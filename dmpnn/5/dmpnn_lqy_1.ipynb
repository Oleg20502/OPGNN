{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "import chemprop\n",
    "from chemprop import data, featurizers, models, nn\n",
    "from chemprop.featurizers.molecule import RDKit2DFeaturizer\n",
    "from chemprop.utils import make_mol\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# logging.getLogger('lightning').setLevel(0)\n",
    "\n",
    "# configure logging at the root level of Lightning\n",
    "# logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "\n",
    "# configure logging on module level, redirect to file\n",
    "# logger = logging.getLogger(\"lightning.pytorch.core\")\n",
    "# logger.addHandler(logging.FileHandler(\"core.log\"))\n",
    "\n",
    "# logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.ERROR)\n",
    "# logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.ERROR)\n",
    "\n",
    "# logging.getLogger(\"lightning.fabric.plugins.environments.slurm\").setLevel(logging.ERROR)\n",
    "# logging.getLogger(\"lightning.pytorch.callbacks.model_checkpoint\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# from skopt import BayesSearchCV\n",
    "# from skopt.space import Real, Categorical, Integer\n",
    "# from skopt.plots import plot_objective\n",
    "# from skopt.plots import plot_convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../train_split_fluor.csv'\n",
    "\n",
    "smiles_columns = ['Chromophore', 'Solvent']\n",
    "target_columns = ['log_q_yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_extra(df, columns):\n",
    "    return df[columns]\n",
    "\n",
    "def dropna(df, columns):\n",
    "    return df.dropna(subset=columns, how='all')\n",
    "\n",
    "def replace_gas(df):\n",
    "    df.loc[df['Solvent'] == 'gas', 'Solvent'] = df['Chromophore']\n",
    "    return df\n",
    "\n",
    "def remove_neg_shift(df):\n",
    "    return df[(df['Stokes shift'] >= 0.0) | (df['Stokes shift'].isna())]\n",
    "\n",
    "def make_log_q_yield(df, eps=1e-5):\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp.loc[df_tmp['Quantum yield'] == 0.0, 'Quantum yield'] = eps\n",
    "    df_tmp.loc[:, 'log_q_yield'] = np.log(df_tmp['Quantum yield'])\n",
    "    return df_tmp\n",
    "\n",
    "def delete_outliers(df, columns):\n",
    "    for column in columns:\n",
    "        print(column)\n",
    "        q1 = df[column].quantile(0.25)\n",
    "        q3 = df[column].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df = df[\n",
    "            ((df[column] > q1 - 1.5 * iqr) & (df[column] < q3 + 1.5 * iqr))\n",
    "            | (df[column].isna())\n",
    "        ]\n",
    "\n",
    "        print(\"left\", q1 - 1.5 * iqr)\n",
    "        print(\"right\", q3 + 1.5 * iqr)\n",
    "        print(\"=\" * 100)\n",
    "    return df\n",
    "\n",
    "def preprocess_train(df):\n",
    "    df = drop_extra(df, smiles_columns + ['Absorption max (nm)', 'Emission max (nm)', 'Stokes shift', 'Quantum yield'])\n",
    "    df = dropna(df, ['Quantum yield'])\n",
    "    df = replace_gas(df)\n",
    "    df = remove_neg_shift(df)\n",
    "    df = make_log_q_yield(df)\n",
    "    df = drop_extra(df, smiles_columns + target_columns)\n",
    "    return df\n",
    "\n",
    "def preprocess_test(df):\n",
    "    df = drop_extra(df, smiles_columns + ['Absorption max (nm)', 'Emission max (nm)', 'Stokes shift', 'Quantum yield'])\n",
    "    df = dropna(df, ['Quantum yield'])\n",
    "    df = replace_gas(df)\n",
    "    df = remove_neg_shift(df)\n",
    "    df = make_log_q_yield(df)\n",
    "    df = drop_extra(df, smiles_columns + target_columns)\n",
    "    return df\n",
    "\n",
    "def rmsd(pred, target):\n",
    "    mask = ~np.isnan(pred) & ~np.isnan(target)\n",
    "    return root_mean_squared_error(pred[mask], target[mask])\n",
    "\n",
    "def r2(pred, target):\n",
    "    mask = ~np.isnan(pred) & ~np.isnan(target)\n",
    "    return np.corrcoef(pred[mask], target[mask])[0, 1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12515, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = preprocess_train(data_df)\n",
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1850, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.read_csv('../test_split_fluor.csv')\n",
    "test_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_clean = preprocess_test(test_data_df)\n",
    "test_data_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiss = data_clean.loc[:, smiles_columns].values\n",
    "ys = data_clean.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smiss = test_data_clean.loc[:, smiles_columns].values\n",
    "test_ys = test_data_clean.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12515, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashurin/soft/chemprop/chemprop/featurizers/molecule.py:52: UserWarning: The RDKit 2D features can deviate signifcantly from a normal distribution. Consider manually scaling them using an appropriate scaler before creating datapoints, rather than using the scikit-learn `StandardScaler` (the default in Chemprop).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "molecule_featurizer = RDKit2DFeaturizer()\n",
    "\n",
    "def generate_mol_features(smiss, save_file, molecule_featurizer=molecule_featurizer):\n",
    "    mols = [make_mol(smis, keep_h=False, add_h=False) for smis in smiss]\n",
    "    extra_datapoint_descriptors = [molecule_featurizer(mol) for mol in tqdm(mols)]\n",
    "    np.savez(save_file, extra_datapoint_descriptors)\n",
    "    return extra_datapoint_descriptors\n",
    "\n",
    "def load_mol_features(save_file):\n",
    "    extra_mol_features = np.load(save_file)\n",
    "    return [extra_mol_features[f\"arr_{i}\"] for i in range(len(extra_mol_features))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_file = 'rdkit_feats_1.npz'\n",
    "# extra_datapoint_descriptors_1 = generate_mol_features(smiss[:, 0], save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = '../gnn_3/rdkit_feats_1.npz'\n",
    "extra_datapoint_descriptors_1 = load_mol_features(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_file = 'rdkit_feats_2.npz'\n",
    "# extra_datapoint_descriptors_2 = generate_mol_features(smiss[:, 1], save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = '../gnn_3/rdkit_feats_2.npz'\n",
    "extra_datapoint_descriptors_2 = load_mol_features(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_file = 'rdkit_feats_1_test.npz'\n",
    "# extra_datapoint_descriptors_1_test = generate_mol_features(test_smiss[:, 0], save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = '../gnn_3/rdkit_feats_1_test.npz'\n",
    "extra_datapoint_descriptors_1_test = load_mol_features(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_file = 'rdkit_feats_2_test.npz'\n",
    "# extra_datapoint_descriptors_2_test = generate_mol_features(test_smiss[:, 1], save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = '../gnn_3/rdkit_feats_2_test.npz'\n",
    "extra_datapoint_descriptors_2_test = load_mol_features(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_indeces = np.loadtxt('good_indeces.txt').astype(np.int64)\n",
    "ind_important_feats = np.loadtxt('ind_important_features_lqy.txt').astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_important_feats = list(range(420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mol_descriptors(d_1, d_2, good_indeces, inds):\n",
    "    extra_datapoint_descriptors = np.concatenate((\n",
    "        d_1,\n",
    "        d_2),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    extra_datapoint_descriptors = extra_datapoint_descriptors[:, good_indeces]\n",
    "    extra_datapoint_descriptors = extra_datapoint_descriptors[:, inds]\n",
    "\n",
    "    return extra_datapoint_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_datapoint_descriptors = get_mol_descriptors(\n",
    "    extra_datapoint_descriptors_1,\n",
    "    extra_datapoint_descriptors_2,\n",
    "    good_indeces,\n",
    "    ind_important_feats\n",
    ")\n",
    "\n",
    "mol_feats_scaler = StandardScaler()\n",
    "extra_datapoint_descriptors = mol_feats_scaler.fit_transform(extra_datapoint_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_datapoint_descriptors_test = get_mol_descriptors(\n",
    "    extra_datapoint_descriptors_1_test,\n",
    "    extra_datapoint_descriptors_2_test,\n",
    "    good_indeces,\n",
    "    ind_important_feats\n",
    ")\n",
    "\n",
    "extra_datapoint_descriptors_test = mol_feats_scaler.transform(extra_datapoint_descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [[data.MoleculeDatapoint.from_smi(smis[0], y, x_d=X_d) \\\n",
    "             for smis, y, X_d in zip(smiss, ys, extra_datapoint_descriptors)]]\n",
    "all_data += [[data.MoleculeDatapoint.from_smi(smis[1]) \\\n",
    "              for smis in smiss]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [[data.MoleculeDatapoint.from_smi(smis[0], y, x_d=X_d) \\\n",
    "              for smis, y, X_d in zip(test_smiss, test_ys, extra_datapoint_descriptors_test)]]\n",
    "test_data += [[data.MoleculeDatapoint.from_smi(smis[1]) \\\n",
    "               for smis in test_smiss]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_to_split_by = 1\n",
    "mols = [d.mol for d in all_data[component_to_split_by]]\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.9, 0.05, 0.05))\n",
    "val_indices += test_indices\n",
    "train_data, val_data, _ = data.split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12515"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]) + len(val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "train_datasets = [data.MoleculeDataset(train_data[i], featurizer) for i in range(len(smiles_columns))]\n",
    "val_datasets = [data.MoleculeDataset(val_data[i], featurizer) for i in range(len(smiles_columns))]\n",
    "test_datasets = [data.MoleculeDataset(test_data[i], featurizer) for i in range(len(smiles_columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mcdset = data.MulticomponentDataset(train_datasets)\n",
    "\n",
    "scaler = train_mcdset.normalize_targets()\n",
    "# extra_datapoint_descriptors_scaler = train_mcdset.normalize_inputs(\"X_d\")\n",
    "\n",
    "val_mcdset = data.MulticomponentDataset(val_datasets)\n",
    "val_mcdset.normalize_targets(scaler)\n",
    "# val_mcdset.normalize_inputs(\"X_d\", extra_datapoint_descriptors_scaler)\n",
    "\n",
    "test_mcdset = data.MulticomponentDataset(test_datasets)\n",
    "# tmp\n",
    "# test_mcdset.normalize_inputs(\"X_d\", extra_datapoint_descriptors_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.build_dataloader(train_mcdset, batch_size=batch_size)\n",
    "val_loader = data.build_dataloader(val_mcdset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = data.build_dataloader(test_mcdset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_dict(model_pt, name):\n",
    "    return {k.replace(f'{name}.', ''): v \\\n",
    "                 for k, v in model_pt.items() \\\n",
    "                 if k.startswith(f'{name}.')}\n",
    "\n",
    "def load_model(ckpt_path=None):\n",
    "    mcmp = nn.MulticomponentMessagePassing(\n",
    "        blocks=[\n",
    "            nn.BondMessagePassing(\n",
    "                    d_h=512,\n",
    "                    dropout=0.1,\n",
    "                    depth=3,\n",
    "                    bias=True\n",
    "                )\n",
    "            for _ in range(len(smiles_columns))],\n",
    "        n_components=len(smiles_columns),\n",
    "    )\n",
    "    \n",
    "    output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "    \n",
    "    ffn_input_dim = mcmp.output_dim + extra_datapoint_descriptors.shape[1]\n",
    "    \n",
    "    \n",
    "    ffn = nn.RegressionFFN(\n",
    "        n_tasks=len(target_columns),\n",
    "        output_transform=output_transform,\n",
    "        input_dim=ffn_input_dim,\n",
    "        hidden_dim=512,\n",
    "        n_layers=4,\n",
    "        dropout=0.5,\n",
    "        activation=\"relu\"\n",
    "    )\n",
    "    \n",
    "    # X_d_transform = nn.ScaleTransform.from_standard_scaler(extra_datapoint_descriptors_scaler[0])\n",
    "    \n",
    "    mpnn = models.MulticomponentMPNN(\n",
    "        mcmp,\n",
    "        nn.MeanAggregation(),\n",
    "        ffn,\n",
    "        batch_norm=True,\n",
    "        warmup_epochs=5,\n",
    "        # init_lr=1e-5,\n",
    "        max_lr= 2 * 1e-4,\n",
    "        final_lr= 5 * 1e-5,\n",
    "        metrics=[nn.metrics.RMSEMetric()],\n",
    "        # X_d_transform=X_d_transform\n",
    "    )\n",
    "\n",
    "    if ckpt_path is not None:\n",
    "        mpnn = mpnn.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    return mpnn\n",
    "\n",
    "def load_pretrained_backbone(ckpt_path=None):\n",
    "    mcmp = nn.MulticomponentMessagePassing(\n",
    "        blocks=[\n",
    "            nn.BondMessagePassing(\n",
    "                    d_h=512,\n",
    "                    dropout=0.1,\n",
    "                    depth=3,\n",
    "                    bias=True\n",
    "                )\n",
    "            for _ in range(len(smiles_columns))],\n",
    "        n_components=len(smiles_columns),\n",
    "    )\n",
    "    \n",
    "    output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "    \n",
    "    ffn_input_dim = mcmp.output_dim + extra_datapoint_descriptors.shape[1]\n",
    "    \n",
    "    \n",
    "    ffn = nn.RegressionFFN(\n",
    "        n_tasks=len(target_columns),\n",
    "        output_transform=output_transform,\n",
    "        input_dim=ffn_input_dim,\n",
    "        hidden_dim=512,\n",
    "        n_layers=4,\n",
    "        dropout=0.5,\n",
    "        activation=\"relu\"\n",
    "    )\n",
    "    \n",
    "    # X_d_transform = nn.ScaleTransform.from_standard_scaler(extra_datapoint_descriptors_scaler[0])\n",
    "    \n",
    "    mpnn = models.MulticomponentMPNN(\n",
    "        mcmp,\n",
    "        nn.MeanAggregation(),\n",
    "        ffn,\n",
    "        batch_norm=True,\n",
    "        warmup_epochs=5,\n",
    "        # init_lr=1e-5,\n",
    "        max_lr= 1e-4,\n",
    "        final_lr= 5 * 1e-5,\n",
    "        metrics=[nn.metrics.RMSEMetric()],\n",
    "        # X_d_transform=X_d_transform\n",
    "    )\n",
    "\n",
    "    if ckpt_path is not None:\n",
    "        checkpoint = torch.load(ckpt_path)['state_dict']\n",
    "        mp_statedict = get_module_dict(checkpoint, 'message_passing')\n",
    "        mpnn.message_passing.load_state_dict(mp_statedict)\n",
    "\n",
    "    return mpnn\n",
    "\n",
    "def freeze_backbone(model):\n",
    "    for param in model.message_passing.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_885928/4170682282.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path)['state_dict']\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = '../gnn_4/model_2/epoch=060-val_loss=0.345.ckpt'\n",
    "\n",
    "mpnn = load_pretrained_backbone(ckpt_path)\n",
    "freeze_backbone(mpnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashurin/soft/chemprop/chemprop/models/multi.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hparams = torch.load(checkpoint_path)[\"hyper_parameters\"]\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = [None, 'model_lqy_1/epoch=147-val_loss=0.420.ckpt'][1]\n",
    "\n",
    "mpnn = load_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnn.max_lr = 1e-4\n",
    "mpnn.final_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params 2539009\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in mpnn.parameters() if p.requires_grad)\n",
    "print(f'Trainable params {pytorch_total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "model_dir = 'model_lqy_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashurin/soft/miniconda3/envs/chemprop/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kashurin/soft/miniconda3/envs/chemprop/lib/pyt ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=model_dir,\n",
    "    filename=\"{epoch:03d}-{val_loss:.3f}\"\n",
    ")\n",
    "\n",
    "earlystopping_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=20,\n",
    "    min_delta=0.0\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"cuda\",\n",
    "    devices=[device],\n",
    "    min_epochs=5,\n",
    "    max_epochs=200,\n",
    "    callbacks=[checkpoint_cb, earlystopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/kashurin/soft/miniconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 1.2 M  | train\n",
      "1 | agg             | MeanAggregation              | 0      | train\n",
      "2 | bn              | BatchNorm1d                  | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN                | 1.3 M  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.156    Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                           |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6190550e4642589ecfcb9476ccea31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                  |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(mpnn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4263, device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping_cb.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kashurin/gnn_5/model_lqy_1/epoch=147-val_loss=0.420.ckpt'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_cb.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /home/kashurin/gnn_5/model_lqy_1/epoch=147-val_loss=0.420.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from the checkpoint at /home/kashurin/gnn_5/model_lqy_1/epoch=147-val_loss=0.420.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca92fa10e96945ea8d83466e454755dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt_path = checkpoint_cb.best_model_path\n",
    "# ckpt_path = '/home/kashurin/gnn_4/model_1/epoch=191-val_loss=0.353.ckpt'\n",
    "\n",
    "mpnn_predict = mpnn\n",
    "\n",
    "with torch.inference_mode():\n",
    "    trainer = pl.Trainer(\n",
    "        logger=None,\n",
    "        enable_progress_bar=True,\n",
    "        devices=[device]\n",
    "    )\n",
    "    testing_preds = trainer.predict(mpnn_predict, test_loader, ckpt_path=ckpt_path)\n",
    "\n",
    "testing_preds = np.concatenate(testing_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSD Log quantum yield: 1.0311842150866124\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSD Log quantum yield: {rmsd(testing_preds, test_ys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Log quantum yield: 0.7201068450450531\n"
     ]
    }
   ],
   "source": [
    "print(f\"R2 Log quantum yield: {r2(testing_preds, test_ys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
